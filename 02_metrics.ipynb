{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "There are multiple metrics that we could use to predict churn. Considering that the dataset is quite balanced between both churn scores, we could use **accuracy**. However, to give the stakeholder more flexibility at which churn probability for a specific client they want to intervene, we decided to use the **ROC AUC Score** instead. To get a simple overview for the outcome of a prediction, we will use a **confusion matrix**.\n",
    "\n",
    "## Confusion Matrix\n",
    "A confusion matrix compares 4 possible cases:\n",
    "1. True positive (TP):\n",
    "- True label: positive\n",
    "- Predicted label: positive\n",
    "   \n",
    "2. True negative (TN):\n",
    "- True label: negative\n",
    "- Predicted label: negative\n",
    "   \n",
    "3. False positive (FP):\n",
    "- True label: negative\n",
    "- Predicted label: positive\n",
    "   \n",
    "4. False negative (FN):\n",
    "- True label: positive\n",
    "- Predicted label: negative\n",
    "\n",
    "A confusion matrix can look like this:   \n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:712/1*Z54JgbS4DUwWSknhDCvNTQ.png\" alt=\"An exemplary confusion matrix\" width=\"500\">\n",
    "  <figcaption>An exemplary confusion matrix.</figcaption>\n",
    "</figure>\n",
    "\n",
    "## ROC\n",
    "ROC stands for **receiver operating characteristic curve** and allows an overview of the relationship between True Positive Cases (TPR) and False Positive Cases (FPR) at all positive classification thresholds. Indeed, the predicted classification as positive or negative directly depends on the given threshold to tend to one direction or the other.\n",
    "\n",
    "### Why should we play around with different thresholds?\n",
    "A lower classification threshold might lead to more TP but also more FP cases (FP cases are outcomes where the true label is negative but predicted as positive). Vice versa a high classification threshold might lead to more TN cases but also more to FN cases. Whether it is useful or not to change the threshold depends on the distribution of predicted probabilities and the specific use cases. For instance it might have a more negative impact for the stakeholder to have many FN cases instead of FP cases and vice versa. This is how a ROC curve can look like:   \n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/Roc_curve.svg/800px-Roc_curve.svg.png\" alt=\"ROC Curve\" width=\"500\">\n",
    "  <figcaption>ROC Curve.</figcaption>\n",
    "</figure>\n",
    "\n",
    "### AUC\n",
    "AUC stands for **area under the curve** and is a measure to get the quality of all possible classification thresholds for predictions of a given dataset. The result is indeed the area under the ROC curve, which is why the metric is called **ROC AUC score**. The max ROC AUC score is 1 and  represents a perfect classifier, while a score of 0.5 represents a random classifier. This plot shows the AUC in gray:\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://developers.google.com/static/machine-learning/crash-course/images/AUC.svg\" alt=\"Area under a ROC curve\" width=\"500\">\n",
    "  <figcaption>Area under a ROC curve.</figcaption>\n",
    "</figure>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
